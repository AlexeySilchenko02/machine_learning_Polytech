{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'о', 'е', 'а', 'т', 'р', 'н', 'и', 'с', 'л', 'к', 'в', 'у', 'м', 'д', '\\n', ',', 'ы', 'б', 'п', 'я', 'ь', 'й', 'г', 'з', 'ч', '.', 'ю', 'х', 'ж', '\\xa0', 'ш', 'П', 'Н', 'ц', 'В', ':', 'С', ';', 'О', 'И', '«', '»', '!', 'Ч', 'Д', 'щ', '?', 'Е', 'Т', 'К', '-', 'А', 'Л', 'У', 'З', 'ф', 'М', 'Р', '—', 'Г', 'Б', 'Х', '1', '(', ')', 'Ж', 'э', 'c', '3', 'ё', '[', ']', '9', 'Я', 'ъ', 'Ц', '4', '8', 'Ы', 'Й', '2', 'Ш']\n"
     ]
    }
   ],
   "source": [
    "#Готовим данные для сети\n",
    "TRAIN_TEXT_FILE_PATH = 'text.txt'\n",
    "\n",
    "with open(TRAIN_TEXT_FILE_PATH, encoding='utf-8') as text_file:\n",
    "    text_sample = text_file.readlines()\n",
    "\n",
    "text_sample = ' '.join(text_sample) #объединение всех строк в одну строку\n",
    "\n",
    "def text_to_seq(text_sample):\n",
    "    char_counts = Counter(text_sample) # подсчет частоты каждого символа\n",
    "    char_counts = sorted(char_counts.items(), key = lambda x: x[1], reverse=True) # сортировка символов по частоте их встречаемости в порядке убывания\n",
    "\n",
    "    sorted_chars = [char for char, _ in char_counts] # извлекаем отсортированные символы и игнорируем их частоту встречаемости\n",
    "    print(sorted_chars)\n",
    "    char_to_idx = {char: index for index, char in enumerate(sorted_chars)} #создание пар значений индек-символ\n",
    "    idx_to_char = {v: k for k, v in char_to_idx.items()} # обратное отображение из индексов обратно в словари\n",
    "    sequence = np.array([char_to_idx[char] for char in text_sample]) # преобразует каждый символ в text_sample в его соответствующий индекс из char_to_idx\n",
    "    #, а затем преобразует этот список в массив NumPy\n",
    "\n",
    "    #последовательность индексов, отображение символа на индекс и отображение индекса на символ.\n",
    "    return sequence, char_to_idx, idx_to_char\n",
    "\n",
    "sequence, char_to_idx, idx_to_char = text_to_seq(text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Генерируем батчи из текста\n",
    "SEQ_LEN = 256 # длинна последовательности обучения\n",
    "BATCH_SIZE = 16 # кол-во сегментов последовательности обрабатывающихся в один раз\n",
    "\n",
    "def get_batch(sequence):\n",
    "    trains = [] # хранение входных данных\n",
    "    targets = [] # хранение входных данных\n",
    "    for _ in range(BATCH_SIZE): # создание партии данных\n",
    "        batch_start = np.random.randint(0, len(sequence) - SEQ_LEN) # Определяется начальный индекс случайного сегмента последовательности для включения в батч.\n",
    "        chunk = sequence[batch_start: batch_start + SEQ_LEN] # Выбирается сегмент последовательности\n",
    "        train = torch.LongTensor(chunk[:-1]).view(-1, 1) # изменяет форму тензора так, чтобы он стал двумерным, с одним столбцом\n",
    "        target = torch.LongTensor(chunk[1:]).view(-1, 1) # метки, которые модель должна предсказывать.\n",
    "        trains.append(train)\n",
    "        targets.append(target)\n",
    "    return torch.stack(trains, dim=0), torch.stack(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Генерация текста\n",
    "def evaluate(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
    "    hidden = model.init_hidden() # инициализация начального скрытого состояния модели\n",
    "    idx_input = [char_to_idx[char] for char in start_text] # преобразование начального текстав список индексов и символов\n",
    "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device) # Преобразование списка индексов в тензор PyTorch\n",
    "    predicted_text = start_text # инициализация переменной для накопления предсказанного текста\n",
    "\n",
    "    _, hidden = model(train, hidden) #Прогон начального ввода через модель для обновления скрытого состояния.\n",
    "\n",
    "    inp = train[-1].view(-1, 1, 1) # Инициализация ввода для модели последним символом начального текста.\n",
    "\n",
    "    # генерация текста\n",
    "    for i in range(prediction_len):\n",
    "        output, hidden = model(inp.to(device), hidden) #Получение предсказания от модели и обновление скрытого состояния.\n",
    "        output_logits = output.cpu().data.view(-1) #Преобразование выхода модели в одномерный массив.\n",
    "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy() # распределение вероятностей для следующего символа.\n",
    "        top_index = np.random.choice(len(char_to_idx), p=p_next) #Выбор следующего символа на основе распределения вероятностей.\n",
    "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device) #Преобразование индекса следующего символа в тензор и подготовка его в качестве входа для следующего шага предсказания.\n",
    "        predicted_char = idx_to_char[top_index] #Преобразование индекса обратно в символ.\n",
    "        predicted_text += predicted_char #Добавление предсказанного символа к накапливаемому тексту.\n",
    "\n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем класс нашей нейросети\n",
    "class TextRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):\n",
    "        super(TextRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Слой кодирования\n",
    "        self.encoder = nn.Embedding(self.input_size, self.embedding_size) # преобразует индексы символов/слов в плотные векторы фиксированного размера.\n",
    "\n",
    "        self.rnn = nn.RNN(self.embedding_size, self.hidden_size, self.n_layers)#Определение слоя RNN с размером встраивания на входе и размером скрытого слоя на выходе.\n",
    "       \n",
    "        self.dropout = nn.Dropout(0.2) #случайным образом обнуляет часть входных функций с вероятностью 0.2 для предотвращения переобучения.\n",
    "        # Полносвязный слой\n",
    "        self.fc = nn.Linear(self.hidden_size, self.input_size) #преобразует выходные данные RNN в размер входного словаря,\n",
    "\n",
    "    #Метод для выполнения прямого распространения входных данных через модель.\n",
    "    def forward(self, x, hidden):\n",
    "        # Встраивание входных данных\n",
    "        x = self.encoder(x).squeeze(2)\n",
    "        # Проход через RNN\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        # Применение dropout\n",
    "        out = self.dropout(out)\n",
    "        # Выходные данные\n",
    "        x = self.fc(out)\n",
    "        return x, hidden\n",
    "\n",
    "    #Метод для инициализации скрытого состояния, который возвращает тензор из нулей соответствующей формы\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device)\n",
    "\n",
    "# Пример использования\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = 1000  # Пример размера входных данных\n",
    "hidden_size = 128  # Пример размера скрытого слоя\n",
    "embedding_size = 300  # Пример размера встраивания\n",
    "model = TextRNN(input_size, hidden_size, embedding_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.451728482246399\n",
      " золотая рыбка, старухе старуха свое старик нево море.\n",
      " «Чего не поклосила не просторить старухе,\n",
      " Старик столся закинее море.\n",
      " «Смила.\n",
      " Не синее просторить старик не пушель старуха рыбка, синема море.\n",
      "Loss: 1.5571012711524963\n",
      "  Прозмула море.\n",
      " Отвечает:\n",
      " «Смилуйся, старуха пряла к невод с тобойПраское просторе.\n",
      " Чтобы тобой,\n",
      " Старик синего море.\n",
      " «Чего тебе на полнощных волнойВ траздные служит он к синему морю,\n",
      " Рассказал о\n",
      "Loss: 1.0058998847007752\n",
      "  Происпугалось».\n",
      " \n",
      " Вот новает золотая рыбка,\n",
      " Лишь хведомые вокой,\n",
      " Хочет быть вольною царицей,\n",
      " Хочет быть вольною царицей,\n",
      " Хочет быть вольною царицей,\n",
      " Хочет быть вольною царицей,\n",
      " Хочет быть воль\n"
     ]
    }
   ],
   "source": [
    "#Создаем нейросеть и обучаем ее\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') # Установка устрофства для вычисления\n",
    "model = TextRNN(input_size=len(idx_to_char), hidden_size=128, embedding_size=128, n_layers=2) \n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # инициализируем функцию потерь, используется стандартная для задачи классификации\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True) # выбираем модель оптимизатора\n",
    "\n",
    "#инициализируем планировщик скорости обучения, который уменьшает скорость обучения если модель не улучшается\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    factor=0.5\n",
    ")\n",
    "\n",
    "n_epochs = 150 #кол-во эпох\n",
    "loss_avg = [] # отслеживание средней скорости потери\n",
    "\n",
    "# цикл обучения\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # переводим модель в режим обучения\n",
    "    train, target = get_batch(sequence) #Получение батча данных для обучения и соответствующих целей\n",
    "    train = train.permute(1, 0, 2).to(device) #Перестановка размерностей батча и перемещение его на устройство\n",
    "    target = target.permute(1, 0, 2).to(device)\n",
    "    hidden = model.init_hidden(BATCH_SIZE) #Инициализация скрытого состояния\n",
    "\n",
    "    #выполнение прямого прохода и расчет потерь\n",
    "    output, hidden = model(train, hidden)\n",
    "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
    "\n",
    "    loss.backward() #обратное распростарнение ошибки\n",
    "    optimizer.step() #шаг оптимизации\n",
    "    optimizer.zero_grad() #обнуление градиентов\n",
    "\n",
    "    loss_avg.append(loss.item()) #добавление текущего значения в список\n",
    "\n",
    "    #каждые 50 эпох оценивает модель и выводит потери\n",
    "    if len(loss_avg) >= 50:\n",
    "        mean_loss = np.mean(loss_avg)\n",
    "        print(f'Loss: {mean_loss}')\n",
    "        scheduler.step(mean_loss)\n",
    "        loss_avg = []\n",
    "        model.eval()\n",
    "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
    "        print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: стариких береговБен богом,\n",
      " Хочет быть волны финские забытой старик отвечает:\n",
      " «Смилуйся, государыня рыбка!\n",
      " Что мне держат.\n",
      " Что мне покою:\n",
      " Из тась,\n",
      " Не дает старик ко старухе воротился золотая рыбка,\n",
      " Ли\n",
      "Perplexity: 1.202928941838911\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden()\n",
    "    idx_input = [char_to_idx[char] for char in start_text]\n",
    "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
    "    predicted_text = start_text\n",
    "    total_log_prob = 0\n",
    "\n",
    "    _, hidden = model(train, hidden)\n",
    "    inp = train[-1].view(-1, 1, 1)\n",
    "\n",
    "    for i in range(prediction_len):\n",
    "        output, hidden = model(inp.to(device), hidden)\n",
    "        output_logits = output.cpu().data.view(-1)\n",
    "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()\n",
    "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
    "        top_prob = p_next[top_index]\n",
    "        total_log_prob += np.log(top_prob)\n",
    "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
    "        predicted_char = idx_to_char[top_index]\n",
    "        predicted_text += predicted_char\n",
    "\n",
    "    return predicted_text, total_log_prob # Возвращение сгенерированного текста и общей логарифмированной вероятности\n",
    "\n",
    "def calculate_perplexity(log_prob, sequence_length):\n",
    "    return np.exp(-log_prob / sequence_length)\n",
    "\n",
    "# Example of usage\n",
    "generated_text, total_log_prob = evaluate(\n",
    "    model,\n",
    "    char_to_idx,\n",
    "    idx_to_char,\n",
    "    temp=0.3,\n",
    "    prediction_len=200,\n",
    "    start_text='старик'\n",
    ")\n",
    "\n",
    "perplexity = calculate_perplexity(total_log_prob, len(generated_text))\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
