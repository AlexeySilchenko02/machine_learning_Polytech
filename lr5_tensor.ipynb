{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные нейронные сети\n",
    " Сильченко Алексей Евгеньевич"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.backend import exp, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "text = open('text.txt', encoding='utf-8').read().lower()\n",
    "tokenizer = Tokenizer(char_level=True) #Таким образом, каждый уникальный символ в тексте будет рассматриваться как отдельный токен.\n",
    "tokenizer.fit_on_texts([text])\n",
    "encoded = tokenizer.texts_to_sequences([text])[0] # Это означает, что токенизатор анализирует текст, определяет все уникальные символы и создает для них внутренний словарь (индекс)\n",
    "vocab_size = len(tokenizer.word_index) + 1 #Вычисляет размер словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание последовательностей\n",
    "sequence_length = 10 #длинна последовательности\n",
    "sequences = [] #список для хранения последовательностей\n",
    "for i in range(sequence_length, len(encoded)): #проход по закодированному тексту\n",
    "    sequences.append(encoded[i - sequence_length:i + 1]) #добавление среза закодированного текста\n",
    "sequences = np.array(sequences) #последовательностей преобразуется в массив NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение на входные и выходные данные\n",
    "#разделяет массив последовательностей на входные данные для модели (X) и целевую переменную (y), \n",
    "#затем преобразует их в формат, подходящий для обучения модели глубокого обучения.\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "X = pad_sequences(X, maxlen=sequence_length)\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание модели\n",
    "model = Sequential() #Инициализация последовательной модели.\n",
    "model.add(Embedding(vocab_size, 50, input_length=sequence_length)) #слой преобразует входные данные, представленные индексами слов, в плотные векторы фиксированного размера\n",
    "model.add(SimpleRNN(100, return_sequences=True))  # добавление слоя со 100 нейронами указывает, что слой должен возвращать последовательность выходов для каждого временного шага\n",
    "#model.add(Dropout(0.2))  \n",
    "model.add(SimpleRNN(100)) #Данный слой обеспечивает сводку информации из предыдущих временных шагов.\n",
    "#model.add(Dropout(0.2)) # Добавление Dropout\n",
    "model.add(Dense(vocab_size, activation='softmax')) #Добавление полносвязного слоя (Dense), преобразует вектор в распределение вероятностей\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "329/329 - 4s - loss: 3.2666 - accuracy: 0.1378 - 4s/epoch - 11ms/step\n",
      "Epoch 2/60\n",
      "329/329 - 1s - loss: 2.7710 - accuracy: 0.2413 - 1s/epoch - 3ms/step\n",
      "Epoch 3/60\n",
      "329/329 - 1s - loss: 2.5175 - accuracy: 0.2930 - 1s/epoch - 3ms/step\n",
      "Epoch 4/60\n",
      "329/329 - 1s - loss: 2.3806 - accuracy: 0.3311 - 1s/epoch - 3ms/step\n",
      "Epoch 5/60\n",
      "329/329 - 1s - loss: 2.2753 - accuracy: 0.3553 - 1s/epoch - 3ms/step\n",
      "Epoch 6/60\n",
      "329/329 - 1s - loss: 2.1787 - accuracy: 0.3799 - 1s/epoch - 4ms/step\n",
      "Epoch 7/60\n",
      "329/329 - 1s - loss: 2.0824 - accuracy: 0.4026 - 1s/epoch - 3ms/step\n",
      "Epoch 8/60\n",
      "329/329 - 1s - loss: 1.9930 - accuracy: 0.4375 - 1s/epoch - 3ms/step\n",
      "Epoch 9/60\n",
      "329/329 - 1s - loss: 1.9056 - accuracy: 0.4573 - 1s/epoch - 3ms/step\n",
      "Epoch 10/60\n",
      "329/329 - 1s - loss: 1.8160 - accuracy: 0.4851 - 1s/epoch - 3ms/step\n",
      "Epoch 11/60\n",
      "329/329 - 1s - loss: 1.7391 - accuracy: 0.5047 - 1s/epoch - 3ms/step\n",
      "Epoch 12/60\n",
      "329/329 - 1s - loss: 1.6543 - accuracy: 0.5279 - 1s/epoch - 3ms/step\n",
      "Epoch 13/60\n",
      "329/329 - 1s - loss: 1.5764 - accuracy: 0.5549 - 1s/epoch - 3ms/step\n",
      "Epoch 14/60\n",
      "329/329 - 1s - loss: 1.5031 - accuracy: 0.5678 - 1s/epoch - 3ms/step\n",
      "Epoch 15/60\n",
      "329/329 - 1s - loss: 1.4277 - accuracy: 0.5921 - 1s/epoch - 3ms/step\n",
      "Epoch 16/60\n",
      "329/329 - 1s - loss: 1.3575 - accuracy: 0.6068 - 1s/epoch - 3ms/step\n",
      "Epoch 17/60\n",
      "329/329 - 1s - loss: 1.2842 - accuracy: 0.6362 - 1s/epoch - 3ms/step\n",
      "Epoch 18/60\n",
      "329/329 - 1s - loss: 1.2156 - accuracy: 0.6468 - 1s/epoch - 3ms/step\n",
      "Epoch 19/60\n",
      "329/329 - 1s - loss: 1.1510 - accuracy: 0.6679 - 1s/epoch - 3ms/step\n",
      "Epoch 20/60\n",
      "329/329 - 1s - loss: 1.0860 - accuracy: 0.6893 - 1s/epoch - 3ms/step\n",
      "Epoch 21/60\n",
      "329/329 - 1s - loss: 1.0268 - accuracy: 0.7065 - 1s/epoch - 3ms/step\n",
      "Epoch 22/60\n",
      "329/329 - 1s - loss: 0.9735 - accuracy: 0.7250 - 1s/epoch - 3ms/step\n",
      "Epoch 23/60\n",
      "329/329 - 1s - loss: 0.9154 - accuracy: 0.7408 - 1s/epoch - 3ms/step\n",
      "Epoch 24/60\n",
      "329/329 - 1s - loss: 0.8608 - accuracy: 0.7568 - 1s/epoch - 3ms/step\n",
      "Epoch 25/60\n",
      "329/329 - 1s - loss: 0.8201 - accuracy: 0.7686 - 1s/epoch - 3ms/step\n",
      "Epoch 26/60\n",
      "329/329 - 1s - loss: 0.7685 - accuracy: 0.7881 - 1s/epoch - 3ms/step\n",
      "Epoch 27/60\n",
      "329/329 - 1s - loss: 0.7224 - accuracy: 0.7971 - 1s/epoch - 3ms/step\n",
      "Epoch 28/60\n",
      "329/329 - 1s - loss: 0.6831 - accuracy: 0.8086 - 1s/epoch - 3ms/step\n",
      "Epoch 29/60\n",
      "329/329 - 1s - loss: 0.6496 - accuracy: 0.8211 - 1s/epoch - 3ms/step\n",
      "Epoch 30/60\n",
      "329/329 - 1s - loss: 0.6095 - accuracy: 0.8301 - 1s/epoch - 3ms/step\n",
      "Epoch 31/60\n",
      "329/329 - 1s - loss: 0.5775 - accuracy: 0.8402 - 1s/epoch - 3ms/step\n",
      "Epoch 32/60\n",
      "329/329 - 1s - loss: 0.5428 - accuracy: 0.8538 - 1s/epoch - 3ms/step\n",
      "Epoch 33/60\n",
      "329/329 - 1s - loss: 0.5176 - accuracy: 0.8583 - 1s/epoch - 3ms/step\n",
      "Epoch 34/60\n",
      "329/329 - 1s - loss: 0.4844 - accuracy: 0.8732 - 1s/epoch - 3ms/step\n",
      "Epoch 35/60\n",
      "329/329 - 1s - loss: 0.4757 - accuracy: 0.8716 - 1s/epoch - 3ms/step\n",
      "Epoch 36/60\n",
      "329/329 - 1s - loss: 0.4561 - accuracy: 0.8778 - 1s/epoch - 3ms/step\n",
      "Epoch 37/60\n",
      "329/329 - 1s - loss: 0.4253 - accuracy: 0.8905 - 1s/epoch - 3ms/step\n",
      "Epoch 38/60\n",
      "329/329 - 1s - loss: 0.4145 - accuracy: 0.8874 - 1s/epoch - 3ms/step\n",
      "Epoch 39/60\n",
      "329/329 - 1s - loss: 0.3992 - accuracy: 0.8933 - 1s/epoch - 3ms/step\n",
      "Epoch 40/60\n",
      "329/329 - 1s - loss: 0.3983 - accuracy: 0.8895 - 1s/epoch - 3ms/step\n",
      "Epoch 41/60\n",
      "329/329 - 1s - loss: 0.3808 - accuracy: 0.8972 - 1s/epoch - 3ms/step\n",
      "Epoch 42/60\n",
      "329/329 - 1s - loss: 0.3468 - accuracy: 0.9111 - 1s/epoch - 3ms/step\n",
      "Epoch 43/60\n",
      "329/329 - 1s - loss: 0.3332 - accuracy: 0.9118 - 1s/epoch - 3ms/step\n",
      "Epoch 44/60\n",
      "329/329 - 1s - loss: 0.3227 - accuracy: 0.9175 - 1s/epoch - 3ms/step\n",
      "Epoch 45/60\n",
      "329/329 - 1s - loss: 0.3384 - accuracy: 0.9066 - 1s/epoch - 3ms/step\n",
      "Epoch 46/60\n",
      "329/329 - 1s - loss: 0.3674 - accuracy: 0.8912 - 1s/epoch - 3ms/step\n",
      "Epoch 47/60\n",
      "329/329 - 1s - loss: 0.3276 - accuracy: 0.9117 - 1s/epoch - 3ms/step\n",
      "Epoch 48/60\n",
      "329/329 - 1s - loss: 0.3087 - accuracy: 0.9171 - 1s/epoch - 3ms/step\n",
      "Epoch 49/60\n",
      "329/329 - 1s - loss: 0.3105 - accuracy: 0.9138 - 1s/epoch - 3ms/step\n",
      "Epoch 50/60\n",
      "329/329 - 1s - loss: 0.2956 - accuracy: 0.9213 - 1s/epoch - 4ms/step\n",
      "Epoch 51/60\n",
      "329/329 - 1s - loss: 0.2654 - accuracy: 0.9304 - 1s/epoch - 3ms/step\n",
      "Epoch 52/60\n",
      "329/329 - 1s - loss: 0.3138 - accuracy: 0.9112 - 1s/epoch - 4ms/step\n",
      "Epoch 53/60\n",
      "329/329 - 1s - loss: 0.3334 - accuracy: 0.9030 - 1s/epoch - 4ms/step\n",
      "Epoch 54/60\n",
      "329/329 - 1s - loss: 0.3002 - accuracy: 0.9147 - 1s/epoch - 3ms/step\n",
      "Epoch 55/60\n",
      "329/329 - 1s - loss: 0.2615 - accuracy: 0.9300 - 1s/epoch - 3ms/step\n",
      "Epoch 56/60\n",
      "329/329 - 1s - loss: 0.2410 - accuracy: 0.9372 - 1s/epoch - 3ms/step\n",
      "Epoch 57/60\n",
      "329/329 - 1s - loss: 0.2483 - accuracy: 0.9320 - 1s/epoch - 3ms/step\n",
      "Epoch 58/60\n",
      "329/329 - 1s - loss: 0.2943 - accuracy: 0.9154 - 1s/epoch - 3ms/step\n",
      "Epoch 59/60\n",
      "329/329 - 1s - loss: 0.3305 - accuracy: 0.9019 - 1s/epoch - 3ms/step\n",
      "Epoch 60/60\n",
      "329/329 - 1s - loss: 0.3304 - accuracy: 0.8991 - 1s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x18691022e10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучение модели\n",
    "model.fit(X, y, epochs=60, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция генерации текста\n",
    "def generate_text(model, tokenizer, input_text, length):\n",
    "    text_generated = input_text  # начинаем с исходной фразы\n",
    "    for _ in range(length):\n",
    "        encoded_input = tokenizer.texts_to_sequences([text_generated[-sequence_length:]])[0] #Получение последних sequence_length символов из text_generated, \n",
    "        #их кодирование в числовые токены с помощью токенизатора.\n",
    "        encoded_input = pad_sequences([encoded_input], maxlen=sequence_length, truncating='pre') #Получение последних sequence_length символов из text_generated, \n",
    "        #их кодирование в числовые токены с помощью токенизатора.\n",
    "        prediction = model.predict(encoded_input, verbose=0) #Получение предсказания от модели для закодированного ввода.\n",
    "        predicted_char_index = np.argmax(prediction) #Определение индекса символа с самой высокой вероятностью в предсказании модели.\n",
    "        predicted_char = tokenizer.index_word[predicted_char_index] #Поиск предсказанного символа по его индексу с помощью обратного индекса токенизатора.\n",
    "        text_generated += predicted_char\n",
    "    return text_generated\n",
    "\n",
    "# Оценка перплексии\n",
    "def calculate_perplexity(model, text, tokenizer, sequence_length):\n",
    "    encoded_text = tokenizer.texts_to_sequences([text])[0] #Подготовка последовательностей из входного текста аналогично предыдущим шагам подготовки данных.\n",
    "    sequences = []\n",
    "    for i in range(sequence_length, len(encoded_text)):\n",
    "        sequences.append(encoded_text[i - sequence_length:i + 1])\n",
    "    sequences = np.array(sequences)\n",
    "    X, y = sequences[:,:-1], sequences[:,-1]\n",
    "    X = pad_sequences(X, maxlen=sequence_length)\n",
    "    y = to_categorical(y, num_classes=vocab_size)\n",
    "    \n",
    "    # Вычисление кросс-энтропии\n",
    "    predictions = model.predict(X) #Получение предсказаний от модели для всех входных последовательностей.\n",
    "    cross_entropy = categorical_crossentropy(y, predictions) #Вычисление кросс-энтропии между истинными значениями y и предсказаниями predictions\n",
    "    perplexity = exp(mean(cross_entropy)) #Расчёт перплексии как экспоненциала средней кросс-энтропии по всем примерам в наборе данных.\n",
    "    return perplexity.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329/329 [==============================] - 1s 2ms/step\n",
      "старика взаше? з рыня рыбка!\n",
      "чтом то стеей будетно, моние моне пока,шего грастоегому старина изба тво ведя рюбядею призин е мет, нак стриматвоводик пыще не рогза, не дабыбозей досидая сунные призались;невел\n",
      "Перплексия: 1.2859749794006348\n"
     ]
    }
   ],
   "source": [
    "# Генерация текста\n",
    "starting_text = \"старик\"\n",
    "generated_text = generate_text(model, tokenizer, starting_text, 200)\n",
    "\n",
    "# Вычисление перплексии\n",
    "perplexity = calculate_perplexity(model, text, tokenizer, sequence_length)\n",
    "\n",
    "print(generated_text)\n",
    "print(f\"Перплексия: {perplexity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
